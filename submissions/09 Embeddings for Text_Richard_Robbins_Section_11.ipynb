{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKsRDH5ZUdfasdv"
   },
   "source": [
    "# Lab 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43534tdfgs-v"
   },
   "source": [
    "In this lab, we'll train models for sentiment classification and experiment with learned embeddings for text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7X58hOMTUH-w"
   },
   "outputs": [],
   "source": [
    "# Import the libraries we'll use below.\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as plotly  # for interactive plots\n",
    "import seaborn as sns  # for nicer plots\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "tf.get_logger().setLevel(\"INFO\")\n",
    "#tf.config.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqppUDpmdptk"
   },
   "source": [
    "## Data for Sentiment Classification\n",
    "\n",
    "In this lab, we'll train a *sentiment* classifier for movie reviews. That is, the input is the text of a movie review and the output is the probability the input was a positive review. The target labels are binary, 0 for negative and 1 for positive.\n",
    "\n",
    "Our data includes 50,000 movie reviews on IMDB. The data comes pre-segmented into train and test splits. The [data loading function](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data) below also splits each input text into tokens (words) and maps the words to integer values. Each input is a sequence of integers corresponding to the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5870,
     "status": "ok",
     "timestamp": 1646684495083,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "s6M-asvhQWV_",
    "outputId": "1aca520b-e1b6-4006-ac44-78ff33ac6da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (25000,)\n",
      "Y_train.shape: (25000,)\n",
      "X_test.shape: (25000,)\n",
      "Y_test.shape: (25000,)\n",
      "First training example data: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "First training example label: 1\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(\n",
    "    path=\"imdb.npz\",\n",
    "    num_words=None,\n",
    "    skip_top=0,\n",
    "    maxlen=None,\n",
    "    seed=113,\n",
    "    start_char=1,\n",
    "    oov_char=2,\n",
    "    index_from=3,\n",
    ")\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"Y_train.shape:\", Y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"Y_test.shape:\", Y_test.shape)\n",
    "\n",
    "print(\"First training example data:\", X_train[0])\n",
    "print(\"First training example label:\", Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyIWiy-4gQK-"
   },
   "source": [
    "So our first training example is a positive review. But that sequence of integer IDs is hard to read. The data loader provides a dictionary mapping words to IDs. Let's create a reverse index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1646684508506,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "HQ-qATkhUj7c",
    "outputId": "eea86a69-fe6a-4cdb-ac8c-9307a5118e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest ID: 88587\n"
     ]
    }
   ],
   "source": [
    "# The imdb dataset comes with an index mapping words to integers.\n",
    "# In the index the words are ordered by frequency they occur.\n",
    "index = imdb.get_word_index()\n",
    "\n",
    "# Because we used index_from=3 (above), setting aside ids below 3 for special\n",
    "# symbols, we need to add 3 to the index values.\n",
    "index = dict([(key, value + 3) for (key, value) in index.items()])\n",
    "\n",
    "# Create a reverse index so we can lookup tokens assigned to each id.\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
    "reverse_index[1] = \"<START>\"  # start of input\n",
    "reverse_index[2] = \"#\"  # out-of-vocabulary (OOV)\n",
    "reverse_index[3] = \"<UNUSED>\"\n",
    "\n",
    "max_id = max(reverse_index.keys())\n",
    "print(\"Largest ID:\", max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h76-b07ehWNQ"
   },
   "source": [
    "Note that our index (and reverse index) have over 88,000 tokens. That's quite a large vocabulary! Let's also write a decoding function for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1646684531998,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "UjobmouHS5Dm",
    "outputId": "29975a48-7fda-4600-bd0c-693c35dd8a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "def decode(token_ids):\n",
    "    \"\"\"Return a string with the decoded text given a list of token ids.\"\"\"\n",
    "    # Try looking up each id in the index, but return '#' (for OOV) if not found.\n",
    "    tokens = [reverse_index.get(i, \"#\") for i in token_ids]\n",
    "\n",
    "    # Connect the string tokens with a space.\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Show the ids corresponding tokens in the first example.\n",
    "print(X_train[0])\n",
    "print(decode(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g47w5CackGBA"
   },
   "source": [
    "### Text Lengths\n",
    "As usual, let's start with some data analysis. How long are the reviews? Is there a difference in length between positive and negative reviews? A histogram will help answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1646684547021,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "kEOgzo8Gk3r7",
    "outputId": "e7446ffd-10c5-4c64-a5bc-7cab81a1d73f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest positive review: 2494\n",
      "Longest negative review: 1571\n"
     ]
    }
   ],
   "source": [
    "# Create a list of lengths for training examples with a positive label.\n",
    "text_lengths_pos = [len(x) for (i, x) in enumerate(X_train) if Y_train[i]]\n",
    "\n",
    "# And a list of lengths for training examples with a negative label.\n",
    "text_lengths_neg = [len(x) for (i, x) in enumerate(X_train) if not Y_train[i]]\n",
    "\n",
    "# The histogram function can take a list of inputs and corresponding labels.\n",
    "plt.hist(\n",
    "    [text_lengths_pos, text_lengths_neg],\n",
    "    bins=20,\n",
    "    range=(0, 1000),\n",
    "    label=[\"positive\", \"negative\"],\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Also check the longest reviews.\n",
    "print(\"Longest positive review:\", max(text_lengths_pos))\n",
    "print(\"Longest negative review:\", max(text_lengths_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3ZE9gpkml3a"
   },
   "source": [
    "---\n",
    "### Exercise 1: Token Counts (8 points)\n",
    "For each of the given tokens, construct a table with the number of positive training examples that include that token and the number of negative training examples that include that token. For reference, here are the counts for the first two tokens:\n",
    "\n",
    "|Token|Pos Count|Neg Count|\n",
    "|-|-|-|\n",
    "|good|4767|4849|\n",
    "|bad|1491|4396|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8YOYo6d01aWI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Pos Count</th>\n",
       "      <th>Neg Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>4767</td>\n",
       "      <td>4849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad</td>\n",
       "      <td>1491</td>\n",
       "      <td>4396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazing</td>\n",
       "      <td>868</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boring</td>\n",
       "      <td>301</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>laugh</td>\n",
       "      <td>525</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cry</td>\n",
       "      <td>231</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token  Pos Count  Neg Count\n",
       "0     good       4767       4849\n",
       "1      bad       1491       4396\n",
       "2  amazing        868        240\n",
       "3   boring        301       1205\n",
       "4    laugh        525        685\n",
       "5      cry        231        114"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"good\", \"bad\", \"amazing\", \"boring\", \"laugh\", \"cry\"]\n",
    "# YOUR CODE HERE\n",
    "\n",
    "result = []\n",
    "\n",
    "for token in tokens:\n",
    "    pos = [\n",
    "        idx\n",
    "        for (idx, review) in enumerate(X_train)\n",
    "        if index[token] in review and Y_train[idx]\n",
    "    ]\n",
    "    neg = [\n",
    "        idx\n",
    "        for (idx, review) in enumerate(X_train)\n",
    "        if index[token] in review and not Y_train[idx]\n",
    "    ]\n",
    "    result.append([token, len(pos), len(neg)])\n",
    "\n",
    "pd.DataFrame(result, columns=[\"Token\", \"Pos Count\", \"Neg Count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhzt-LnQ1m8w"
   },
   "source": [
    "## Feature Representation\n",
    "Consider the difference between the pixel features we used for image classification and the text features we are now dealing with.\n",
    "\n",
    "An image had 784 pixel positions. At each position, there is a single value in [0,1] (after normalization).\n",
    "\n",
    "In contrast, a review has a variable number of ordered tokens (up to 2494 in the training examples). Each token occurs in a particular position. We can think of the token positions much like the 784 pixel positions, except that some of the trailing positions are empty, since review lengths vary.  At each token position, there is a single token, one of the 88587 entries in the vocabulary. So we can think of a review as a (2500, 90000) matrix: At each of ~2500 token positions, we have 1 of ~90000 token ids.\n",
    "\n",
    "This representation would have 2500 * 90000 = 225 million features -- quite a lot more complexity than the images, though as you'll see below, we will make some simplifying assumptions, reducing both the number of token positions and the number of vocabulary items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm_F5JmWyfko"
   },
   "source": [
    "### Padding and Reduced Length\n",
    "As is clear from the length histogram, the current representation of the review text is a variable-length array. Since fixed-length arrays are easier to work with in Tensorflow, let's add special padding tokens at the end of each review until they are all the same length.\n",
    "\n",
    "We'll also use this operation to limit the number of token positions by truncating all reviews to a specified length. In the code below, as an example, we pad all training inputs to length 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1646684609342,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "a4ou8bSUCWOx",
    "outputId": "be4bc9e1-47aa-4f82-e23b-569f8d6128a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train[0]: 218\n",
      "Length of X_train_padded[0]: 300\n",
      "[    1    14    22    16    43   530   973  1622  1385    65   458  4468\n",
      "    66  3941     4   173    36   256     5    25   100    43   838   112\n",
      "    50   670 22665     9    35   480   284     5   150     4   172   112\n",
      "   167 21631   336   385    39     4   172  4536  1111    17   546    38\n",
      "    13   447     4   192    50    16     6   147  2025    19    14    22\n",
      "     4  1920  4613   469     4    22    71    87    12    16    43   530\n",
      "    38    76    15    13  1247     4    22    17   515    17    12    16\n",
      "   626    18 19193     5    62   386    12     8   316     8   106     5\n",
      "     4  2223  5244    16   480    66  3785    33     4   130    12    16\n",
      "    38   619     5    25   124    51    36   135    48    25  1415    33\n",
      "     6    22    12   215    28    77    52     5    14   407    16    82\n",
      " 10311     8     4   107   117  5952    15   256     4 31050     7  3766\n",
      "     5   723    36    71    43   530   476    26   400   317    46     7\n",
      "     4 12118  1029    13   104    88     4   381    15   297    98    32\n",
      "  2071    56    26   141     6   194  7486    18     4   226    22    21\n",
      "   134   476    26   480     5   144    30  5535    18    51    36    28\n",
      "   224    92    25   104     4   226    65    16    38  1334    88    12\n",
      "    16   283     5    16  4472   113   103    32    15    16  5345    19\n",
      "   178    32     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "def pad_data(sequences, max_length):\n",
    "    # Keras has a convenient utility for padding a sequence.\n",
    "    # Also make sure we get a numpy array rather than an array of lists.\n",
    "    return np.array(\n",
    "        list(\n",
    "            tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                sequences, maxlen=max_length, padding=\"post\", value=0\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Pad and truncate to 300 tokens.\n",
    "X_train_padded = pad_data(X_train, max_length=300)\n",
    "\n",
    "# Check the padded output.\n",
    "print(\"Length of X_train[0]:\", len(X_train[0]))\n",
    "print(\"Length of X_train_padded[0]:\", len(X_train_padded[0]))\n",
    "print(X_train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFEmcwBjL4e_"
   },
   "source": [
    "### Reduced Vocabulary\n",
    "We also want to be able to limit the vocabulary size. Since our padding function produces fixed-length sequences in a numpy matrix, we can use clever numpy indexing to efficiently replace all token ids larger than some value with the designated out-of-vocabulary (OOV) id.\n",
    "\n",
    "In the code below, as an example, we'll keep just token ids less than 1000, replacing all others with OOV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1646684634116,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "21qpyEgGNQeB",
    "outputId": "148ce890-4376-4c5a-f96a-9fb50cbe9538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1  14  22  16  43 530 973   2   2  65 458   2  66   2   4 173  36 256\n",
      "   5  25 100  43 838 112  50 670   2   9  35 480 284   5 150   4 172 112\n",
      " 167   2 336 385  39   4 172   2   2  17 546  38  13 447   4 192  50  16\n",
      "   6 147   2  19  14  22   4   2   2 469   4  22  71  87  12  16  43 530\n",
      "  38  76  15  13   2   4  22  17 515  17  12  16 626  18   2   5  62 386\n",
      "  12   8 316   8 106   5   4   2   2  16 480  66   2  33   4 130  12  16\n",
      "  38 619   5  25 124  51  36 135  48  25   2  33   6  22  12 215  28  77\n",
      "  52   5  14 407  16  82   2   8   4 107 117   2  15 256   4   2   7   2\n",
      "   5 723  36  71  43 530 476  26 400 317  46   7   4   2   2  13 104  88\n",
      "   4 381  15 297  98  32   2  56  26 141   6 194   2  18   4 226  22  21\n",
      " 134 476  26 480   5 144  30   2  18  51  36  28 224  92  25 104   4 226\n",
      "  65  16  38   2  88  12  16 283   5  16   2 113 103  32  15  16   2  19\n",
      " 178  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "<START> this film was just brilliant casting # # story direction # really # the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same # # as myself so i loved the fact there was a real # with this film the # # throughout the film were great it was just brilliant so much that i # the film as soon as it was released for # and would recommend it to everyone to watch and the # # was amazing really # at the end it was so sad and you know what they say if you # at a film it must have been good and this definitely was also # to the two little # that played the # of # and paul they were just brilliant children are often left out of the # # i think because the stars that play them all # up are such a big # for the whole film but these children are amazing and should be # for what they have done don't you think the whole story was so # because it was true and was # life after all that was # with us all # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n"
     ]
    }
   ],
   "source": [
    "def limit_vocab(sequences, max_token_id, oov_id=2):\n",
    "    \"\"\"Replace token ids greater than or equal to max_token_id with the oov_id.\"\"\"\n",
    "    reduced_sequences = np.copy(sequences)\n",
    "    reduced_sequences[reduced_sequences >= max_token_id] = oov_id\n",
    "    return reduced_sequences\n",
    "\n",
    "\n",
    "# Reduce vocabulary to 1000 tokens.\n",
    "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
    "print(X_train_reduced[0])\n",
    "\n",
    "# Decode to see what this looks like in tokens. Note the '#' for OOVs.\n",
    "print(decode(X_train_reduced[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d24mOPC6ybC4"
   },
   "source": [
    "### One-hot Encoding\n",
    "Our current feature representations are **sparse**. That is, we only keep track of the token ids that are present in the input. A **one-hot** encoding replaces a value like 22 (corresponding to 'film') with an array with a single 1 at position 22 and zeros everywhere else. This will be very memory-inefficient, but we'll do it anyway for clarity.\n",
    "\n",
    "As discussed above, let's dramatically reduce both the number of token positions (review length) and the number of token ids (vocabulary). We'll clip each review after 20 tokens (so 2500 -> 20) and keep only the most common 1000 tokens (so 90000 -> 1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1777,
     "status": "ok",
     "timestamp": 1646684668163,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "EXzkqVL3Jufj",
    "outputId": "69a06ea3-61e2-4d2b-ed57-69c51ee2c5f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_one_hot shape: (25000, 20, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Keras has a util to create one-hot encodings.\n",
    "X_train_padded = pad_data(X_train, max_length=20)\n",
    "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
    "X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n",
    "print(\"X_train_one_hot shape:\", X_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5RvIN4w66Ej"
   },
   "source": [
    "Note the shape of the one-hot encoded features. For each of our 25000 training examples, we have a 20 x 1000 matrix. That is, for each of 20 token positions, we have a vector of 1000 elements containing a single 1 and 999 zeros.\n",
    "\n",
    "We can think of these 1000-dimensional one-hot arrays as **embeddings**. Each token in the input has a 1000-dimensional representation. But because of the one-hot setup, the distance between each pair of tokens is the same ([1,0,0,...], [0,1,0,...], etc.). By contrast, learned embeddings result in meaningful distances between pairs of tokens. We'll get to that soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "296Cnt647b5c"
   },
   "source": [
    "## Logistic Regression with One-Hot Encodings\n",
    "Let's start with something familiar -- logistic regression. Since our feature representation is in 2 dimensions (20 x 1000), we need to flatten it to pass it to Keras (remember we did this with the pixel data too). Let's try two strategies for flattening.\n",
    "\n",
    "1. Flatten by *concatenating* (as we did with pixels), turning (20 x 1000) data into (20000,) data. The result is a separate feature for each token at each position.\n",
    "2. Flatten by *averaging* over token positions, turning (20 x 1000) data into (1000,) data. The result is an array with average token counts, ignoring position.\n",
    "\n",
    "NOTE: Our prior assignments have used the standard Stochastic Gradient Descent (SGD) optimizer to compute the gradient from an estimate of the loss (based on the current mini-batch). There are many alternative optimizers. Here we'll use the **Adam** optimizer, which sometimes gives better results. One key characteristic of Adam is that it effectively uses a different learning rate for each parameter rather than a fixed learning rate as in SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6m6eebM-0dUW"
   },
   "outputs": [],
   "source": [
    "def build_onehot_model(average_over_positions=False):\n",
    "    \"\"\"Build a tf.keras model for one-hot data.\"\"\"\n",
    "    # Clear session and remove randomness.\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    if average_over_positions:\n",
    "        # This layer averages over the first dimension of the input by default.\n",
    "        model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "    else:\n",
    "        # Concatenate.\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=1,  # output dim (for binary classification)\n",
    "            activation=\"sigmoid\",  # sigmoid activation for classification\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",  # this is a classification task\n",
    "        optimizer=\"adam\",  # fancy optimizer\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY3W_1-OSZ2X"
   },
   "source": [
    "Now let's try fitting the model to our training data and check performance metrics on the validation (held-out) data. But first, here's a function for plotting the learning curves given the training history object we get from Keras.\n",
    "\n",
    "**Note to Grader** I replaced the loss history plotting function with a more comprehensive reporting function that plots loss and accuracy during training and displays final training and validation accuracy as well as the number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cOVmajSuMjN6"
   },
   "outputs": [],
   "source": [
    "def history_report(model, history, caption=None):\n",
    "    if caption:\n",
    "        print(caption)\n",
    "    history = pd.DataFrame(history.history)\n",
    "    plot_loss_history(history)\n",
    "    plot_accuracy_history(history)\n",
    "\n",
    "    final_training_accuracy, final_validation_accuracy = get_final_accuracy(history)\n",
    "\n",
    "    print(f\"Final training accuracy: {final_training_accuracy:.4f}\")\n",
    "    print(f\"Final validation accuracy: {final_validation_accuracy:.4f}\")\n",
    "    print(f\"Model parameter count: {get_total_parameters(model):,}\")\n",
    "\n",
    "\n",
    "def plot_loss_history(history):\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(range(0, len(history[\"loss\"] + 1)))\n",
    "    plt.plot(history[\"loss\"], label=\"training\", marker=\"o\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"validation\", marker=\"o\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_accuracy_history(history):\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(range(0, len(history[\"accuracy\"] + 1)))\n",
    "    plt.plot(history[\"accuracy\"], label=\"training\", marker=\"o\")\n",
    "    plt.plot(history[\"val_accuracy\"], label=\"validation\", marker=\"o\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_final_accuracy(history):\n",
    "    return history[\"accuracy\"].values[-1], history[\"val_accuracy\"].values[-1]\n",
    "\n",
    "\n",
    "def get_total_parameters(model):\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    summarystring = \"\\n\".join(stringlist)\n",
    "    total_parameter_string = re.search(\"Total params: (.*)\\n\", summarystring).group(1)\n",
    "    return int(total_parameter_string.replace(\",\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 22140,
     "status": "ok",
     "timestamp": 1646684718388,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "MyE4PgX70_op",
    "outputId": "3de05dfa-f372-4b77-ba43-f212c2f47c1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 16:27:05.693906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-30 16:27:05.715537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-30 16:27:05.715735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-30 16:27:05.716291: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-30 16:27:05.716789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-30 16:27:05.716987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-30 16:27:05.717156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-30 16:27:06.025199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-30 16:27:06.025404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-30 16:27:06.025565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-30 16:27:06.025699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 567 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-06-30 16:27:16.686623: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.68GiB (rounded to 1800000000)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-06-30 16:27:16.686715: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc\n",
      "2022-06-30 16:27:16.686753: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256): \tTotal Chunks: 5, Chunks in use: 5. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 28B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.686780: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.686807: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2022-06-30 16:27:16.686831: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.686852: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.686874: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.686896: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.686918: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.686940: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.686961: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.686984: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.687006: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.687027: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.687048: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.687071: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.687093: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.687114: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.687135: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.687158: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.687180: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.687213: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0. 567.37MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-30 16:27:16.687241: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 1.68GiB was 256.00MiB, Chunk State: \n",
      "2022-06-30 16:27:16.687274: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]   Size: 567.37MiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 256B | Requested Size: 8B | in_use: 1 | bin_num: -1\n",
      "2022-06-30 16:27:16.687294: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 594935808\n",
      "2022-06-30 16:27:16.687317: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc710000000 of size 256 next 1\n",
      "2022-06-30 16:27:16.687337: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc710000100 of size 1280 next 2\n",
      "2022-06-30 16:27:16.687356: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc710000600 of size 256 next 3\n",
      "2022-06-30 16:27:16.687374: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc710000700 of size 256 next 4\n",
      "2022-06-30 16:27:16.687392: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc710000800 of size 256 next 5\n",
      "2022-06-30 16:27:16.687409: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc710000900 of size 256 next 6\n",
      "2022-06-30 16:27:16.687428: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7fc710000a00 of size 594933248 next 18446744073709551615\n",
      "2022-06-30 16:27:16.687447: I tensorflow/core/common_runtime/bfc_allocator.cc:1065]      Summary of in-use Chunks by size: \n",
      "2022-06-30 16:27:16.687470: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 5 Chunks of size 256 totalling 1.2KiB\n",
      "2022-06-30 16:27:16.687491: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2022-06-30 16:27:16.687513: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 2.5KiB\n",
      "2022-06-30 16:27:16.687534: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 594935808 memory_limit_: 594935808 available bytes: 0 curr_region_allocation_bytes_: 1189871616\n",
      "2022-06-30 16:27:16.687562: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats: \n",
      "Limit:                       594935808\n",
      "InUse:                            2560\n",
      "MaxInUse:                         2560\n",
      "NumAllocs:                           6\n",
      "MaxAllocSize:                     1280\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-06-30 16:27:16.687584: W tensorflow/core/common_runtime/bfc_allocator.cc:468] *___________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m build_onehot_model()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Fit the model.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# one-hot training data\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# corresponding binary labels\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of passes through the training data\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# mini-batch size\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# use a fraction of the examples for validation\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# display some progress output during training\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Convert the return value into a DataFrame so we can see the train loss\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# and binary accuracy after every epoch.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# history_ = pd.DataFrame(history.history)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# plot_loss_history(history)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1134\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1128\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mcoordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[1;32m   1129\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), \\\n\u001b[1;32m   1132\u001b[0m      training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1133\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[0;32m-> 1134\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1383\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1382\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1138\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1135\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1137\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:230\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    220\u001b[0m              x,\n\u001b[1;32m    221\u001b[0m              y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m              shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    228\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    229\u001b[0m   \u001b[38;5;28msuper\u001b[39m(TensorLikeDataAdapter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 230\u001b[0m   x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_process_tensorlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m   sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    232\u001b[0m       sample_weights, sample_weight_modes)\n\u001b[1;32m    234\u001b[0m   \u001b[38;5;66;03m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1031\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[1;32m   1029\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m-> 1031\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_numpy_and_scipy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1026\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1024\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[1;32m   1025\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[0;32m-> 1026\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[1;32m   1028\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1430\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m   1369\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1370\u001b[0m   \u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \n\u001b[1;32m   1372\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1430\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1436\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1435\u001b[0m   \u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1436\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m      \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1566\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1561\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1562\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1563\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1566\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1569\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:52\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    176\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:283\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    282\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 283\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    286\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:308\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    307\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    105\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "model = build_onehot_model()\n",
    "\n",
    "# Fit the model.\n",
    "history = model.fit(\n",
    "    x=X_train_one_hot,  # one-hot training data\n",
    "    y=Y_train,  # corresponding binary labels\n",
    "    epochs=5,  # number of passes through the training data\n",
    "    batch_size=64,  # mini-batch size\n",
    "    validation_split=0.1,  # use a fraction of the examples for validation\n",
    "    verbose=1,  # display some progress output during training\n",
    ")\n",
    "\n",
    "# Convert the return value into a DataFrame so we can see the train loss\n",
    "# and binary accuracy after every epoch.\n",
    "# history_ = pd.DataFrame(history.history)\n",
    "\n",
    "# plot_loss_history(history)\n",
    "\n",
    "print()\n",
    "history_report(model, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuCh9aQPv7F_"
   },
   "source": [
    "---\n",
    "### Exercise 2: Comparing logistic regression models (8 points)\n",
    "Train the one-hot model using both the concatenating and the averaging strategies and compare the results. Let's call these *LR-C* (Logistic Regression Concatenating) and *LR-A* (Logistic Regression Averaging). Then answer the following questions:\n",
    "\n",
    "1. What are the final training and validation accuracies for LR-C and LR-A?\n",
    "2. How many parameters are there in each model?\n",
    "3. Would you say that either model is overfitting? Why or why not?\n",
    "4. Briefly describe how LR-C differs from LR-A. How do you explain the relationship between their respective validation accuracy results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_c = build_onehot_model(average_over_positions=False)\n",
    "lr_a = build_onehot_model(average_over_positions=True)\n",
    "\n",
    "print(\"Train Concatenating Model:\")\n",
    "\n",
    "lr_c_history = lr_c.fit(\n",
    "    x=X_train_one_hot,  # one-hot training data\n",
    "    y=Y_train,  # corresponding binary labels\n",
    "    epochs=5,  # number of passes through the training data\n",
    "    batch_size=64,  # mini-batch size\n",
    "    validation_split=0.1,  # use a fraction of the examples for validation\n",
    "    verbose=1,  # display some progress output during training\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"Train Averaging Model:\")\n",
    "\n",
    "lr_a_history = lr_a.fit(\n",
    "    x=X_train_one_hot,  # one-hot training data\n",
    "    y=Y_train,  # corresponding binary labels\n",
    "    epochs=5,  # number of passes through the training data\n",
    "    batch_size=64,  # mini-batch size\n",
    "    validation_split=0.1,  # use a fraction of the examples for validation\n",
    "    verbose=1,  # display some progress output during training\n",
    ")\n",
    "\n",
    "print()\n",
    "history_report(lr_c, lr_c_history, \"Concatenating Model\")\n",
    "print()\n",
    "history_report(lr_a, lr_a_history, \"Averaging Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEAN5BejHc__"
   },
   "source": [
    "*Written answers:*\n",
    "\n",
    "1. Final training and validation accuracies for LR-C and LR-A.\n",
    "| Model         | Training | Validation |\n",
    "|---------------|----------|------------|\n",
    "| Concatenating (LR-C) | 0.8100   | 0.6816     |\n",
    "| Averaging (LR-A)     | 0.6958   | 0.6888     |\n",
    "\n",
    "\n",
    "2. The concatenating model has `20,001` parameters and the averaging model has `1,001` parameters.\n",
    "\n",
    "3. Overfitting is evidenced by training accuracy exceeding test accuracy by a meaningful degree.  It demonstrates that the model does not generalize well.  The accuracy plots above show that the concatenating model is overfitting while the averaging model is not.\n",
    "\n",
    "4. The primary difference between the two models is the number of features and the accompanying very large difference in the number of model parameters.  The additional features present in the concatening model makes that model more prone to overfitting.  By reducing dimensionality, the averaging model is less prone to overfitting.  The difference in validation accuracy between the two models is immaterial with the averaging model doing slightly better.  Reducing dimensionality without losing the predictive power of the underlying features is what makes this approach work, which is similar in spirit to what we see with principal component analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJIBRqK7lsjG"
   },
   "source": [
    "## Logistic Regression with Embeddings\n",
    "Next, let's train model that replaces one-hot representations of each token with learned embeddings.\n",
    "\n",
    "The code below uses a Keras Embedding layer, which expects to receive a sparse (rather than one-hot) representation. That is, it expects a (padded) sequence of token ids; for each id, it looks up the corresponding embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ho6uOeCaBs2e"
   },
   "outputs": [],
   "source": [
    "def build_embeddings_model(\n",
    "    average_over_positions=False, vocab_size=1000, sequence_length=20, embedding_dim=2\n",
    "):\n",
    "    \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
    "    # Clear session and remove randomness.\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if average_over_positions:\n",
    "        # This layer averages over the first dimension of the input by default.\n",
    "        model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "    else:\n",
    "        # Concatenate.\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=1,  # output dim (for binary classification)\n",
    "            activation=\"sigmoid\",  # apply the sigmoid function!\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyhoEjAiFSNB"
   },
   "source": [
    "Try training the model as before. We'll use the averaging strategy rather than the concatenating strategy for dealing with the token sequence. That is, we'll look up embedding vectors for each token. Then we'll average them to produce a single vector. Then we'll traing a logistic regression with that vector as input to predict the binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 5490,
     "status": "ok",
     "timestamp": 1646684762935,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "uYUE5UwkxoU8",
    "outputId": "20a4c237-8c49-4b24-9ea1-e0f325cb9908"
   },
   "outputs": [],
   "source": [
    "model = build_embeddings_model(\n",
    "    average_over_positions=True, vocab_size=1000, sequence_length=20, embedding_dim=2\n",
    ")\n",
    "history = model.fit(\n",
    "    x=X_train_reduced,  # our sparse padded training data\n",
    "    y=Y_train,  # corresponding binary labels\n",
    "    epochs=5,  # number of passes through the training data\n",
    "    batch_size=64,  # mini-batch size\n",
    "    validation_split=0.1,  # use a fraction of the examples for validation\n",
    "    verbose=1,  # display some progress output during training\n",
    ")\n",
    "\n",
    "print()\n",
    "history_report(model, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3k__61hFnag"
   },
   "source": [
    "---\n",
    "### Exercise 3: Experiments with embeddings (8 points)\n",
    "Train 6 models with embedding sizes in [2,4,8,16,32,64], keeping other settings fixed. Use the averaging strategy rather than the concatenating strategy.\n",
    "\n",
    "1. Construct a table with the training and validation accuracies of each model (after 5 training epochs).\n",
    "2. Compute the number of parameters in each model.\n",
    "3. Do learned embeddings appear to provide improved performance over the one-hot encoding? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    build_embeddings_model(average_over_positions=True, embedding_dim=pow(2, x + 1))\n",
    "    for x in range(6)\n",
    "]\n",
    "\n",
    "history_list = [\n",
    "    model.fit(\n",
    "        x=X_train_reduced,  # our sparse padded training data\n",
    "        y=Y_train,  # corresponding binary labels\n",
    "        epochs=5,  # number of passes through the training data\n",
    "        batch_size=64,  # mini-batch size\n",
    "        validation_split=0.1,  # use a fraction of the examples for validation\n",
    "        verbose=1,  # display some progress output during training\n",
    "    )\n",
    "    for model in model_list\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7t46ZdX2ofd"
   },
   "source": [
    "*Written answers:*\n",
    "\n",
    "1. A table with the training and validation accuracies of each model (after 5 training epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [\n",
    "    (\n",
    "        pow(2, idx + 1),\n",
    "        history_list[idx].history[\"accuracy\"][-1],\n",
    "        history_list[idx].history[\"val_accuracy\"][-1],\n",
    "    )\n",
    "    for idx in range(len(history_list))\n",
    "]\n",
    "\n",
    "pd.DataFrame(result, columns=[\"embeddings\", \"training accuracy\", \"validation accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the number of parameters in each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual Calculation**\n",
    "\n",
    "We have limited the vocabulary to 1,000 tokens and the number of tokens in each review to 20, so when we average the features over positions, each embedding in the model is associated with 1,000 features.  \n",
    "The total number of parameters for `n` embeddings is: `1000n + n + 1` = `1001n + 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total parameters = embeddings + embeddings*1000 + 1\n",
    "result = [\n",
    "    (pow(2, idx + 1), pow(2, idx + 1) * 1001 + 1)\n",
    "    for (idx, model) in enumerate(model_list)\n",
    "]\n",
    "\n",
    "pd.DataFrame(result, columns=[\"embeddings\", \"total parameters\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confirm Manual Calculation by Examining the Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [\n",
    "    (pow(2, idx + 1), get_total_parameters(model))\n",
    "    for (idx, model) in enumerate(model_list)\n",
    "]\n",
    "\n",
    "pd.DataFrame(result, columns=[\"embeddings\", \"total parameters\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Do learned embeddings appear to provide improved performance over the one-hot encoding? Why?\n",
    "\n",
    "The learned embeddings models have a validation accuracy of .71 to .73 compared with .68 to .69 for the one-hot encoding-based models.  In other words, we see accuracy improving by approximately three to four percent when moving to learned embeddings from one-hot encoding.  The one-hot encodings look at tokens in isolation and derive no predictive power from the proximity to other tokens.  Learned embeddings impound information about context, surrounding tokens.  The additional information represented by learned embeddings is responsible for the enhanced performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2dWOuxqKHA6"
   },
   "source": [
    "## Inspecting Learned Embeddings\n",
    "Let's retrieve the learned embedding parameters from the trained model and plot the token embeddings.\n",
    "\n",
    "The model layers in a Keras Sequential model are stored as a list and the embeddings are the first layer. We can use the get_weights() function to get a numpy array with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1646684774106,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "bfsbGSwkaFjo",
    "outputId": "d9d74723-012f-464f-a9f3-8a41d8178868"
   },
   "outputs": [],
   "source": [
    "# Display the model layers.\n",
    "display(model.layers)\n",
    "\n",
    "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
    "embeddings = model.layers[0].get_weights()[0]\n",
    "display(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apPWscNwcXTE"
   },
   "source": [
    "Now we'll use a fancy plotting tool called *plotly* to show the embeddings with hovertext so you can move your mouse over the points to see the corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1646684778338,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "5RZMTrA0KttL",
    "outputId": "f5ec9b65-84bb-4c28-c673-cf86a87112df"
   },
   "outputs": [],
   "source": [
    "def plot_2d_embeddings(embeddings, id_start=1, count=100):\n",
    "    # Get 1st and 2nd embedding dims for the desired tokens.\n",
    "    x1 = embeddings[id_start : id_start + count, 0]\n",
    "    x2 = embeddings[id_start : id_start + count, 1]\n",
    "\n",
    "    # Get the corresponding words from the reverse index (for labeling).\n",
    "    tokens = [reverse_index[i] for i in range(id_start, id_start + count)]\n",
    "\n",
    "    # Plot with the plotly library.\n",
    "    data = plotly.Scatter(\n",
    "        x=x1,\n",
    "        y=x2,\n",
    "        text=tokens,\n",
    "        mode=\"markers\",\n",
    "        textposition=\"bottom left\",\n",
    "        hoverinfo=\"text\",\n",
    "    )\n",
    "    fig = plotly.Figure(\n",
    "        data=[data], layout=plotly.Layout(title=\"Word Embeddings\", hovermode=\"closest\")\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Very frequent tokens tend to be more syntactic than semantic, so let's plot\n",
    "# some rarer words.\n",
    "plot_2d_embeddings(embeddings, id_start=500, count=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3Mm8MjRcZ20"
   },
   "source": [
    "---\n",
    "### Exercise 4: Interpretting Embeddings (8 points)\n",
    "Notice that the 2-D embeddings fall in a narrow diagonal band.\n",
    "\n",
    "1. Have the learned embeddings separated positive and negative words? What is the most negative word? Does this make sense?\n",
    "2. Give 2 examples of words that seem to have surprising embedding values and try to explain their positions. For example, what's going on with the tokens '7', '8', and '9'?\n",
    "3. The embedding for 'crazy' is very close to (0,0). Explain what this means in terms of the model's output.\n",
    "4. Can you explain what you think the 2 learned embedding dimensions mean, if anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_qAAvvo2y3t"
   },
   "source": [
    "*Written answers:*\n",
    "\n",
    "1. The learned embeddings separate positive from negative words.  The most negative word is `avoid`.  That makes sense.  It is a clear and unambiguous expression of negative sentiment.\n",
    "2. Examples of words that seem to have surprising embedding values:\n",
    "   - At first glance, it seems odd that tokens `7`, `8` and `9` are positive (and especially odd that `9` has a lower positive association than `7` or `8`).  However, in connection with any ratings system, such as IMDB, those numbers are typically used to express a high rating on a ten point scale, as in, \"I rate this movie an 8.\"  \n",
    "   - I am surprised by the positive association for `today`.  It could be that people who are very enthusiastic about movies tend to reference the fact that they saw the movie 'today' and felt moved to provide a positive rating swiftly.  On the other hand, I'm not sure why someone who disliked the movie wouldn't be similarly motived to expression their then-recent dissatisfaction.  \n",
    "    - I do not understand why `write` and `effort` have modest negative connotations.  Someone can praise or criticize the writing or the apparent effort or lack thereof.  Perhaps people who tend to discusss the writing or effort usually do so in a negative sense and when they praise a movie, it is with reference to other attributes.\n",
    "3. The embedding for `crazy` at nearly (0,0) means that that word provides little to no predictive information about sentiment, that it is just as likely to be used in a positive as a negative fashion and therefor has little impact on the model's output.\n",
    "4. I think that the x-axis is a weight for negative sentiment and the y-axis for positive sentiment.  Those two numbers are co-linear which would explain why we have a reasonably straight line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXCitmUvxfwb"
   },
   "source": [
    "## Scaling Up!\n",
    "Remember how we limited our input sequences to 20 tokens and 1000 vocabulary entries? Let's see how well we can do using more data and bigger models (more parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKZDEGS7xzr6"
   },
   "source": [
    "### Exercise 5: Improve Results (8 points)\n",
    "Using pieces of code from above, set up and train a model that improves the validation accuracy to at least 80%. You should include the following elements:\n",
    "\n",
    "1. Truncate and pad input to the desired length.\n",
    "2. Limit vocabulary to the desired size.\n",
    "3. Set up a model using embeddings.\n",
    "4. Add an additional layer or layers (after the embeddings layer and before the output layer).\n",
    "5. Evaluate on the test data. Remember to apply the same pre-processing to the test data. You can use model.evaluate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekbJ4sIq2hID"
   },
   "outputs": [],
   "source": [
    "def build_experiment_model(\n",
    "    vocab_size=1000, sequence_length=20, hidden_units=10, embedding_dim=2\n",
    "):\n",
    "    \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
    "    # Clear session and remove randomness.\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # This layer averages over the first dimension of the input by default.\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "    # Hidden layer\n",
    "    model.add(tf.keras.layers.Dense(units=hidden_units, activation=\"relu\"))\n",
    "\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=1,  # output dim (for binary classification)\n",
    "            activation=\"sigmoid\",  # apply the sigmoid function!\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def experiment(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_features,\n",
    "    sequence_length=20,\n",
    "    vocab_size=1000,\n",
    "    hidden_units=1000,\n",
    "    embedding_dim=2,\n",
    "    verbose=1\n",
    "):\n",
    "    train_padded = pad_data(train_features, max_length=sequence_length)\n",
    "    train_padded_and_reduced = limit_vocab(train_padded, max_token_id=vocab_size)\n",
    "\n",
    "    test_padded = pad_data(test_features, max_length=sequence_length)\n",
    "    test_padded_and_reduced = limit_vocab(test_padded, max_token_id=vocab_size)\n",
    "\n",
    "    model = build_experiment_model(\n",
    "        vocab_size=vocab_size,\n",
    "        sequence_length=sequence_length,\n",
    "        hidden_units=hidden_units,\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=train_padded_and_reduced,\n",
    "        y=train_labels,\n",
    "        epochs=5,\n",
    "        batch_size=64,\n",
    "        validation_split=0.1,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if verbose == 1: history_report(model, history)\n",
    "    return (model, train_padded_and_reduced, test_padded_and_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    experimental_model,\n",
    "    experimental_train_features,\n",
    "    experimental_test_features,\n",
    ") = experiment(\n",
    "    X_train, Y_train, X_test, sequence_length=268, vocab_size=3829, hidden_units=100, embedding_dim=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_model.evaluate(experimental_test_features, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_model.summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbG/uKJC3itEUb58OjZyV3",
   "name": "09 Embeddings for Text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "35d15a1cd8dadd50efd7dc67d5d4964ef4d6df162d72ab0491992c60321de6b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
