{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indoor-commodity",
   "metadata": {},
   "source": [
    "## Week 2: Linear Regression - part I\n",
    "\n",
    "Instructor: Cornelia Ilin <br>\n",
    "Email: cilin@ischool.berkeley.edu <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-boston",
   "metadata": {},
   "source": [
    "#### ``Objectives``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-request",
   "metadata": {},
   "source": [
    "1. Supervised ML: predict continous target variables with regression analysis\n",
    "\n",
    "2. Gradient Descent for linear regression (stochastic and batch)\n",
    "\n",
    "3. Use AI/ML to track the progression of diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-sodium",
   "metadata": {},
   "source": [
    "#### ``Motivation``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-capability",
   "metadata": {},
   "source": [
    "1. Start with simple predictive models (if realistic/possible)\n",
    "\n",
    "2. AI/ML can help predict what types of people are likely to be afflicted with diabetes and how the disease is likely to progress based upon a person’s weight and lifestyle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-married",
   "metadata": {},
   "source": [
    "#### ``Data``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-referral",
   "metadata": {},
   "source": [
    "[Paper](https://tibshirani.su.domains/ftp/lars.pdf): Efron et al. (2004): “Least Angle Regression,” Annals of Statistics (with discussion), 407–499.\n",
    "\n",
    "[Data description](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html): 10 baseline variables (age, sex, body mass index, average blood pressure, and six blood serum measurements) and the response of interest (a quantitative measure of disease progression one year after baseline readings) for n = 442 diabetes patients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-market",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacterial-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ML/stats\n",
    "from mlxtend.plotting import heatmap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-material",
   "metadata": {},
   "source": [
    "### Step 2: Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "knowing-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_regplot(X, y, y_pred):\n",
    "    plt.scatter(X, y, c='steelblue', edgecolor='white', s=70)\n",
    "    plt.plot(X, y_pred, color='black', lw=2)    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-while",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Read data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt',\n",
    "    delimiter='\\t'\n",
    ")\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-series",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Data preprocessing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-contemporary",
   "metadata": {},
   "source": [
    "``rename columns``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\n",
    "    'age', 'sex', 'bmi', 'map',\n",
    "    'tc', 'ldl', 'hdl', 'tch',\n",
    "    'ltg', 'glu', 'disease_progression']\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-brighton",
   "metadata": {},
   "source": [
    "``split data into training and test``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define outcome #\n",
    "##################\n",
    "y = df['disease_progression'].values\n",
    "\n",
    "# define features #\n",
    "###################\n",
    "X = df[['age', 'sex', 'bmi', 'map', 'tc', 'ldl', 'hdl', 'tch', 'ltg', 'glu']].values\n",
    "\n",
    "# split #\n",
    "#########\n",
    "split = (0.7,0.3) #70% training and 30% test\n",
    "shuffle = np.random.permutation(np.arange(y.shape[0])) # very important to shuffle the data. Why?\n",
    "X, y = X[shuffle], y[shuffle]\n",
    "\n",
    "splits = np.multiply(len(y), split).astype(int) \n",
    "X_train, X_test = np.split(X, [splits[0]])\n",
    "y_train, y_test = np.split(y, [splits[0]])\n",
    "\n",
    "\n",
    "# df for y_train and X_train #\n",
    "##############################\n",
    "# easier for EDA later on\n",
    "df_train = pd.concat(\n",
    "    [pd.DataFrame(y_train), pd.DataFrame(X_train)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# rename columns\n",
    "df_train.columns=['disease_progression', 'age', 'sex', 'bmi', 'map',\n",
    "    'tc', 'ldl', 'hdl', 'tch',\n",
    "    'ltg', 'glu']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-trigger",
   "metadata": {},
   "source": [
    "``feature and outcome scalling for optimal performance``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-driver",
   "metadata": {},
   "source": [
    "Can be done in two ways:\n",
    "\n",
    "normalization: rescaling of the features to a range of [0,1]. Also called min-max scaling. Mostly used when we need values in a bounded interval (you can do this manually, but you can also use the MinMaxScaler() method that you can find in sklearn.preprocessing module).\n",
    "\n",
    "standardization: center the feature columns at mean 0 and std 1. Prefered over normalization, as it preservers useful information about outliers and makes the algorithm less sensitive to them. This is what we will be implementing today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize\n",
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "\n",
    "X_train_std = sc_x.fit(X_train).transform(X_train)\n",
    "X_test_std = sc_x.fit(X_train).transform(X_test)\n",
    "\n",
    "y_train_std = sc_y.fit(np.expand_dims(y_train, axis=1)).transform(np.expand_dims(y_train, axis=1)).flatten()\n",
    "y_test_std = sc_y.fit(np.expand_dims(y_train, axis=1)).transform(np.expand_dims(y_test, axis=1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-prototype",
   "metadata": {},
   "source": [
    "Note how we fit the StandardScaler() only on the training data, and use the mean and sd parameters to transform the test data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-compact",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Exploratory Data Analysis (EDA)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-magazine",
   "metadata": {},
   "source": [
    "Prior to training a model it is important to perform exploratory data analysis. The idea is to detect any missing data, the presence of outliers, the feature distribution, and the relationship between features and outcome. Note that typically there is a back and forth between Step 4 and 5, depending on how satisfied you are with the results of the EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-notification",
   "metadata": {},
   "source": [
    "`` check if any missing values``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-article",
   "metadata": {},
   "source": [
    "``correlation matrix of outcome and feature variables``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = df_train.corr().values\n",
    "hm = heatmap(\n",
    "    cm,\n",
    "    row_names=df_train.columns,\n",
    "    column_names=df_train.columns,\n",
    "    figsize=(8, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-wedding",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\"> *Question:*</span> Which features seem to be most informative to predict diabetes progression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-beginning",
   "metadata": {},
   "source": [
    "``histogram of features and outcome``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-publicity",
   "metadata": {},
   "source": [
    "Let's focus on the outcome variable (disease_progression) and BMI for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['disease_progression', 'bmi']\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    # plot histogram\n",
    "    ax.hist(df_train[columns[idx]])\n",
    "    # set xlabel\n",
    "    ax.set_xlabel(columns[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-monkey",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6: Analysis - Linear Regression with one variable\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-picnic",
   "metadata": {},
   "source": [
    "We will begin with a single-variable linear regression to predict 'disease_progression' from 'bmi'.\n",
    "\n",
    "The idea behind linear regression is to obtain the best-fitting straight line through the examples in the training data.\n",
    "\n",
    "There are two ways you can fit the training data:\n",
    " - using a closed-form solution: $(X'X)^{-1}(X'y)$\n",
    " - using an iterative solution: gradient descent (stochastic, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-speaker",
   "metadata": {},
   "source": [
    "Today we will be focusing on **gradient descent** and we will implement this algorithm from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features of interest\n",
    "temp_X_train_std = X_train_std[:,2] # choose only BMI\n",
    "\n",
    "# exapand dimension so that temp_X_train_std is a 2D array\n",
    "temp_X_train_std = np.expand_dims(temp_X_train_std, axis=1) \n",
    "\n",
    "# define shape of temp_X_train_std\n",
    "temp_shape_X_train_std = temp_X_train_std.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-community",
   "metadata": {},
   "source": [
    "``Linear Regression, w/ Batch Gradient Descent``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./images/linear.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-velvet",
   "metadata": {},
   "source": [
    "Let j represent features, j = [1,...,m]\n",
    "\n",
    "Let i represent observations, i = [1, ..., n]\n",
    "\n",
    "\n",
    "``Net input function``:\n",
    "\n",
    "z = $w_{0} \\cdot 1 + w_{2} \\cdot x_{1} + w_{2} \\cdot x_{2} + ... + w_{m} \\cdot x_{m}$\n",
    "\n",
    "``Linear activation function``:\n",
    "\n",
    "$\\phi(z)= w_{0} \\cdot 1 + w_{2} \\cdot x_{1} + w_{2} \\cdot x_{2} + ... + w_{m} \\cdot x_{m}$\n",
    "\n",
    "\n",
    "`` Compute error``:\n",
    "\n",
    "$(y\\_actual_{i} - \\phi(z_{i}))$, , where $\\phi(z_{i})$ is $y\\_pred_{i}$\n",
    "\n",
    "``Update weights``:\n",
    "\n",
    "$w_{j} := w_{j} + \\Delta w_{j}, \\forall j \\in {1,...,m}$\n",
    "\n",
    "$\\Delta w_{j} = \\eta \\sum _{i}(y\\_actual_{i} - \\phi(z_{i}))\\cdot x_{i,j}, \\forall j \\in {1,...,m}, \\forall i \\in {1,...,n}$\n",
    "\n",
    "\n",
    "``Threshold function``:\n",
    "\n",
    "$\\tau(z) = w_{0} \\cdot 1 + w_{2} \\cdot x_{1} + w_{2} \\cdot x_{2} + ... + w_{m} \\cdot x_{m}$\n",
    "\n",
    "\n",
    "`` Note on Batch Gradient Descent``:\n",
    "\n",
    "The weights are updated using **all training examples** (see the summation operator in the equation $\\Delta w_{j}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: initalize hyperparameters\n",
    "eta = 0.001\n",
    "n_iter = 10\n",
    "random_state = 1\n",
    "\n",
    "# Step 2: initalize weights (to small random numbers)\n",
    "rgen = np.random.RandomState(random_state)\n",
    "w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + temp_shape_X_train_std)\n",
    "\n",
    "# Step 3: initialize cost lists\n",
    "cost_ = []\n",
    "\n",
    "# Step 4: compute net input, linear activation function, update weights, track cost\n",
    "def fit(X, y):\n",
    "    for iter_ in range(n_iter):\n",
    "        # compute the net input function\n",
    "        net_input = np.dot(X, w_[1:]) + w_[0]\n",
    "\n",
    "        # compute the linear activation function; this gives us y_pred\n",
    "        temp_y_pred = net_input\n",
    "        \n",
    "        # compute error\n",
    "        errors = (y - temp_y_pred)\n",
    "        #print(y, temp_y_pred)\n",
    "\n",
    "        # update weights (notice how weights are updated using all training examples!)\n",
    "        w_[1:] += eta * X.T.dot(errors)\n",
    "        w_[0] += eta * errors.sum()\n",
    "\n",
    "        # compute cost (we find weights by minimizing costs; in the Linear model the cost is SSE/2)\n",
    "        cost = (errors**2).sum()/2.0  \n",
    "        cost_.append(cost)\n",
    "        \n",
    "    return cost_\n",
    "\n",
    "# Step 5: compute the threshold (linear) function\n",
    "def predict(X):\n",
    "    net_input = np.dot(X, w_[1:]) + w_[0]\n",
    "    \n",
    "    # threshold function\n",
    "    y_pred = net_input\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-angle",
   "metadata": {},
   "source": [
    "Model fit and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cost_ = fit(temp_X_train_std, y_train_std)\n",
    "y_train_std_pred = predict(temp_X_train_std)\n",
    "\n",
    "# plot cost\n",
    "plt.plot((range(1, len(cost_) + 1)), cost_, marker='o');\n",
    "plt.xlabel('epochs (n_iter)');\n",
    "plt.ylabel('SSE (cost)'); # number of times y_actual != y_pred\n",
    "\n",
    "plt.title('Adeline convergence with BGD. Learning rate = ' + str(eta));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-header",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:', mean_squared_error(y_train_std, y_train_std_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-confusion",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\"> *Exercise:*</span> \n",
    "\n",
    "Try increasing the number of iterations and see what happens.\n",
    "\n",
    "Then see what happens if you change the learning rate.\n",
    "\n",
    "If the learning rate is too large (set eta = 0.02): instead of minimizing the cost function, the error become larger after each epoch. This happens because we overshoot the global minimum.\n",
    "\n",
    "If the learning rate is too small (set eta = 0.0001): the cost decreases but the learning rate is so small that the algo requires lots of epochs to converge (this can be very computational expensive with large datasets!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./images/effect_of_eta.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-chase",
   "metadata": {},
   "source": [
    "The left subfigure shows that if choose the \"right learning rate\", the cost decreases gradually, moving in the direction of the global minimum.\n",
    "\n",
    "The right subfigure shows what happens if we choose a learning rate that is too large - we miss the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-steps",
   "metadata": {},
   "source": [
    "``Linear Regression, w/ Stochastic Gradient Descent``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-italic",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\"> *Update weights:*</span>\n",
    "\n",
    "$w_{j} := w_{j} + \\Delta w_{j}, \\forall j \\in {1,...,m}$\n",
    "\n",
    "$\\Delta w_{j} = \\eta (y\\_actual_{i} - \\phi(z_{i}))\\cdot x_{i,j}, \\forall j \\in {1,...,m}, \\forall i \\in {1,...,n}$\n",
    "\n",
    "`` Note on Stochastic Gradient Descent``:\n",
    "\n",
    "The weights are updated using **one training example at a time** (no summation operator in the equation $\\Delta w_{j}$)\n",
    "\n",
    "\n",
    "As a result, the SGD typically reaches convergence much faster than BGD because of the more frequent weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: initalize hyperparameters\n",
    "eta = 0.001\n",
    "n_iter = 10\n",
    "random_state = 1\n",
    "\n",
    "# Step 2: initalize weights (to small random numbers)\n",
    "rgen = np.random.RandomState(random_state)\n",
    "w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + temp_shape_X_train_std)\n",
    "\n",
    "# Step 3: initialize cost lists\n",
    "cost_ = []\n",
    "\n",
    "\n",
    "# Step 4: compute net input, linear activation function, update weights, track cost\n",
    "def fit(X, y):\n",
    "    for iter_ in range(n_iter):\n",
    "        cost = []\n",
    "        # shuffle the data after each iteration to avoid repetitive cycles when we optimize the cost function\n",
    "        shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "        X, y = X[shuffle], y[shuffle]\n",
    "        \n",
    "        \n",
    "        # compute the net input function\n",
    "        net_input = np.dot(X, w_[1:]) + w_[0]\n",
    "\n",
    "        # compute the linear activation function; this gives us y_pred\n",
    "        temp_y_pred = net_input\n",
    "        \n",
    "        # iterate through all i in X,y\n",
    "        for idx, (xi, y_actual) in enumerate(zip(X, y)):\n",
    "            # compute error\n",
    "            errors = (y_actual - temp_y_pred[idx])\n",
    "            \n",
    "\n",
    "            # update weights (notice how weights are updated using one training example at a time!)\n",
    "            w_[1:] += eta * xi.dot(errors)\n",
    "            w_[0] += eta * errors\n",
    "\n",
    "            # compute cost (we find weights by minimizing costs; in the Linear model the cost is SSE/2)\n",
    "            cost.append(errors**2/2.0)\n",
    "            \n",
    "        avg_cost = sum(cost)/len(y)\n",
    "        cost_.append(avg_cost)\n",
    "    return cost_\n",
    "\n",
    "# Step 5: compute the threshold (linear) function; it's just the net input function in the linear model\n",
    "def predict(X):\n",
    "    net_input = np.dot(X, w_[1:]) + w_[0]\n",
    "    # threshold function\n",
    "    y_pred = net_input\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-sarah",
   "metadata": {},
   "source": [
    "Model fit and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cost_ = fit(temp_X_train_std, y_train_std)\n",
    "y_train_std_pred = predict(temp_X_train_std)\n",
    "\n",
    "# plot cost\n",
    "plt.plot((range(1, len(cost_) + 1)), cost_, marker='o');\n",
    "plt.xlabel('epochs (n_iter)');\n",
    "plt.ylabel('SSE (cost)'); # number of times y_actual != y_pred\n",
    "\n",
    "plt.title('Adeline convergence with BGD. Learning rate = ' + str(eta));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-crisis",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:', mean_squared_error(y_train_std, y_train_std_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-memphis",
   "metadata": {},
   "source": [
    "``The regression line``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_regplot(temp_X_train_std, y_train_std, y_train_std_pred)\n",
    "plt.xlabel(' Body Mass Index (standardized)')\n",
    "plt.ylabel('Disease progression (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-account",
   "metadata": {},
   "source": [
    "As you can see, the linear regression line reflects the general trend that diabetes progression tends to increase with the BMI. This observation makes sense but the data also tells us that BMI does not explain disease progression very well in many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-cookbook",
   "metadata": {},
   "source": [
    "---\n",
    "TO DOs in the breakout rooms:\n",
    "\n",
    "   - [a] What is the MSE on the test data?  <br>\n",
    "   - [b] How would you improve the MSE of the model? Implement your idea.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
