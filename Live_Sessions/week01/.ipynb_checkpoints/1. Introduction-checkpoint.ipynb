{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1: Introduction\n",
    "\n",
    "Instructor: Cornelia Ilin <br>\n",
    "Email: cilin@ischool.berkeley.edu <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``Objectives``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduce you to a typical workflow for using Machine Learning in predictive modeling.\n",
    "\n",
    "2. Read through the commands, try making changes, and make sure you understand how the functions work.\n",
    "\n",
    "3. Focus on making your code as organized and readable as possible. Use lots of comments!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``Motivation``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Machine learning is an exciting field that can help you turn massive data into knowledge.\n",
    "\n",
    "2. Use powerful algorithms to learn patterns from data and make predictions about future events.\n",
    "\n",
    "3. Easyto break into the field, thanks to the many open source libraries (sklearn, tensorflow, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``Data``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generated using numpy\n",
    "2. Size = (200, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard \n",
    "import numpy as np\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# images\n",
    "from IPython.display import Image\n",
    "\n",
    "# prediction\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# This tells matplolib not to try opening a new window for each plot\n",
    "%matplotlib inline\n",
    "\n",
    "# silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set working directory (CHANGE HERE)\n",
    "import os\n",
    "os.chdir('/Users/cilin/Postdoc/teaching/cilin-coursework2/Live_Sessions/week01/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will generate our own data, y and X, by using a random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a randomizer seeds (this will ensure the results are the same each time)\n",
    "np.random.seed(100)\n",
    "\n",
    "# set len(X)\n",
    "len_X = 50\n",
    "\n",
    "# generate evenly spaced X values in [0, 1]. Set len(X) = 20\n",
    "X = np.linspace(0, 1, len_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question 1: What is the data type of X?\")\n",
    "print(type(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a \"true\" function (a piece of a cosine curve) that we will try to approximate with a model\n",
    "true_function = lambda x: np.cos(1.5 * np.pi * x)\n",
    "\n",
    "# try this function out. Notice that you can apply it to a scalar, an array, or you can use it even in pandas\n",
    "print(true_function(0))\n",
    "print(true_function(0.5))\n",
    "print(true_function(np.array([0, 0.5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate true y values\n",
    "y = true_function(X)\n",
    "\n",
    "# print the values of y to the nearest hundredth\n",
    "print (['%.2f' %i for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add random noise to y\n",
    "# the randn function samples random numbers from the standard Normal distribution\n",
    "# multiplying adjusts the standard deviation of the distribution\n",
    "noise = np.random.randn(len_X) * 0.2\n",
    "y += noise\n",
    "\n",
    "# print the noise-added values of y for comparison.\n",
    "print (['%.2f' %i for i in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to predict y, using the feature vector X. \n",
    "\n",
    "In this course, our outputs (y) will always be 1-dimensional. Our inputs (X) will usually have more than 1 dimension. Today, for simplicity, we have just a single feature. \n",
    "\n",
    "Since the machine learning classes in sklearn expect input feature vectors, we need to turn each input x in X into a feature vector [x]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``labels and features``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y -> labels <br>\n",
    "X -> features (1 in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X into a vector (an alternative command is X = np.transpose(X))\n",
    "X = X[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``create training and test sets``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "       X, y, test_size=0.30\n",
    ")\n",
    "\n",
    "# print size\n",
    "print('Size of X_train', X_train.shape)\n",
    "print('Size of y_train', y_train.shape)\n",
    "\n",
    "print('Size of X_test', X_test.shape)\n",
    "print('Size of y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fit\n",
    "lm = LinearRegression(fit_intercept = True)\n",
    "lm.fit(X_train, y_train)\n",
    "print ('Estimated function: y = %.2f + %.2fx' %(lm.intercept_, lm.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating a cosine function with a linear model doesn't work so well. By adding polynomial transformations of our feature(s), we can fit more complex functions. This is often called polynomial (nonlinear) regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Nonlinear model (poly degree==4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create polinomial transformations\n",
    "poly = PolynomialFeatures(degree=4, include_bias=False)\n",
    "X4_train = poly.fit_transform(X_train)\n",
    "print(X4_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fit\n",
    "lm4 = LinearRegression(fit_intercept=True)\n",
    "lm4.fit(X4_train, y_train)\n",
    "\n",
    "print ('Estimated function: y = %.2f + %.2fx + %.2fx^2 + %.2fx^3 + %.2fx^4' %(lm4.intercept_, lm4.coef_[0], lm4.coef_[1], lm4.coef_[2], lm4.coef_[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Nonlinear model (poly degree==15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create polinomial transformations\n",
    "poly = PolynomialFeatures(degree=15, include_bias=False)\n",
    "X15_train = poly.fit_transform(X_train)\n",
    "print(X15_train[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fit\n",
    "lm15 = LinearRegression(fit_intercept=True)\n",
    "lm15.fit(X15_train, y_train)\n",
    "print('Print intercept:', lm15.intercept_)\n",
    "print('\\nPrint slope coefficients:', lm15.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">What is the estimated function?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 4, 15]\n",
    "\n",
    "# Initialize a new plot and set plot size\n",
    "plt.figure(figsize=(14, 4)) \n",
    "\n",
    "for i in range(len(degrees)):\n",
    "    # create sublots that are all on the same row\n",
    "    ax = plt.subplot(1, len(degrees), i+1)\n",
    "    \n",
    "    # create the polynomial feature vector (or matrix)\n",
    "    poly = PolynomialFeatures(degree = degrees[i], include_bias = False)\n",
    "    temp_X_train = poly.fit_transform(X_train)\n",
    "    temp_X_test = poly.fit_transform(X_test)\n",
    "    \n",
    "    # model fit\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(temp_X_train, y_train)\n",
    "    lm_yhat_train = lm.predict(temp_X_train)\n",
    "    lm_yhat_test = lm.predict(temp_X_test)\n",
    "    \n",
    "    \n",
    "    # plot the true function\n",
    "    #sb.lineplot(np.squeeze(X_train), np.squeeze(true_function(X_train)), label=\"True function\");\n",
    "    \n",
    "    # plot the true function with noise added\n",
    "    sb.scatterplot(np.squeeze(X_train), y_train, label=\"Function with noise\");\n",
    "\n",
    "    # Show the fitted function for the linear model using training data\n",
    "    sb.lineplot(np.squeeze(X_train), lm_yhat_train, color='black', label='Evaluation, train data')\n",
    "    \n",
    "    sb.lineplot(np.squeeze(X_test), lm_yhat_test, color='red', label='Evaluation, test data')\n",
    "\n",
    "    \n",
    "    # Add labels, title, legend to the plot\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((-.05, 1.05))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree %d\" %degrees[i])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lesson here is that we are interested in the model that generalizes well. \n",
    "\n",
    "Clearly, the degree 1 model, while very small (only 2 parameters), doesn't fit the observed training data well. The degree 15 model fits the observed training data extremely well, but is unlikely to generalize to new (test) data.\n",
    "\n",
    "This is a case of \"over-fitting\", which often happens when we try to estimate too many parameters from just a few examples. The degree 4 model appears to be a good blend of small model size and good generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Exaplain what we did in this notebook by using the \"Roadmap for building ML systems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./images/roadmap_ml_systems.png', width = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
